{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "betstudy_url='https://www.betstudy.com/soccer-stats/c/england/premier-league/d/results/2018-2019/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get line ups spider needs to iterate over all games on the web page, getting the date and team names to validate then traveling along the hyperlink to get the line up web page.\n",
    "\n",
    "Once on the line up web page iterate over both lineups and take all names.\n",
    "\n",
    "Final dataframe should have two rows per game, with 20 columns. One for team name, one for date and 18 for line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_spider(url):\n",
    "    '''\n",
    "    input: url\n",
    "    output: two dictionaries, one with the player names the other with their positions\n",
    "    as both are scraped in parallel the location of the name is the location of the position\n",
    "    e.g. player at lineups[\"1starter_5\"][7] has position lineups_pos[\"1starter_5\"][7].\n",
    "    \n",
    "    function that iterates over table rows\n",
    "    also runs other functions that collects data\n",
    "    '''\n",
    "    #gets soup of url\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    #gets list of each table row\n",
    "    table=soup.find('table', class_='schedule-table').find_all('tr', class_='rounds')\n",
    "    \n",
    "    \n",
    "    lineups={\n",
    "        'date':[],\n",
    "        'team1':[],\n",
    "        'team2':[],\n",
    "        '1starter_1':[],\n",
    "        '1starter_2':[],\n",
    "        '1starter_3':[],\n",
    "        '1starter_4':[],\n",
    "        '1starter_5':[],\n",
    "        '1starter_6':[],\n",
    "        '1starter_7':[],\n",
    "        '1starter_8':[],\n",
    "        '1starter_9':[],\n",
    "        '1starter_10':[],\n",
    "        '1starter_11':[],\n",
    "        '1sub_1':[],\n",
    "        '1sub_2':[],\n",
    "        '1sub_3':[],\n",
    "        '1sub_4':[],\n",
    "        '1sub_5':[],\n",
    "        '1sub_6':[],\n",
    "        '1sub_7':[],\n",
    "        '2starter_1':[],\n",
    "        '2starter_2':[],\n",
    "        '2starter_3':[],\n",
    "        '2starter_4':[],\n",
    "        '2starter_5':[],\n",
    "        '2starter_6':[],\n",
    "        '2starter_7':[],\n",
    "        '2starter_8':[],\n",
    "        '2starter_9':[],\n",
    "        '2starter_10':[],\n",
    "        '2starter_11':[],\n",
    "        '2sub_1':[],\n",
    "        '2sub_2':[],\n",
    "        '2sub_3':[],\n",
    "        '2sub_4':[],\n",
    "        '2sub_5':[],\n",
    "        '2sub_6':[],\n",
    "        '2sub_7':[],\n",
    "        'referee':[],\n",
    "        'assistant_1':[],\n",
    "        'assistant_2':[],\n",
    "        'fourth':[]\n",
    "    }\n",
    "\n",
    "    lineups_pos={\n",
    "        '1starter_1':[],\n",
    "        '1starter_2':[],\n",
    "        '1starter_3':[],\n",
    "        '1starter_4':[],\n",
    "        '1starter_5':[],\n",
    "        '1starter_6':[],\n",
    "        '1starter_7':[],\n",
    "        '1starter_8':[],\n",
    "        '1starter_9':[],\n",
    "        '1starter_10':[],\n",
    "        '1starter_11':[],\n",
    "        '1sub_1':[],\n",
    "        '1sub_2':[],\n",
    "        '1sub_3':[],\n",
    "        '1sub_4':[],\n",
    "        '1sub_5':[],\n",
    "        '1sub_6':[],\n",
    "        '1sub_7':[],\n",
    "        '2starter_1':[],\n",
    "        '2starter_2':[],\n",
    "        '2starter_3':[],\n",
    "        '2starter_4':[],\n",
    "        '2starter_5':[],\n",
    "        '2starter_6':[],\n",
    "        '2starter_7':[],\n",
    "        '2starter_8':[],\n",
    "        '2starter_9':[],\n",
    "        '2starter_10':[],\n",
    "        '2starter_11':[],\n",
    "        '2sub_1':[],\n",
    "        '2sub_2':[],\n",
    "        '2sub_3':[],\n",
    "        '2sub_4':[],\n",
    "        '2sub_5':[],\n",
    "        '2sub_6':[],\n",
    "        '2sub_7':[],\n",
    "    }\n",
    "    \n",
    "    for game in tqdm_notebook(table):\n",
    "        #collect data for game into two rows, one per team\n",
    "        \n",
    "        #scrape date for both teams\n",
    "        date = get_date(game)\n",
    "        lineups['date'].append(date)\n",
    "        \n",
    "        #get team names\n",
    "        lineups['team1'].append(get_team(game, 'left'))\n",
    "        lineups['team2'].append(get_team(game, 'right'))\n",
    "        \n",
    "        #get soup for lineup page\n",
    "        lineup_soup=get_lineup_soup(game)\n",
    "        \n",
    "        #gets the 18 players for each team\n",
    "        name_pos_dict = get_lineup(lineup_soup)\n",
    "        \n",
    "        #parsing the name_pos_dict into\n",
    "        #the lineups and lineups_pos dicts\n",
    "        for key in name_pos_dict.keys():\n",
    "            lineups[key].append(name_pos_dict[key][0]) #player_name\n",
    "            lineups_pos[key].append(name_pos_dict[key][1]) #player_pos\n",
    "            \n",
    "        #gets the 4 refs for each game\n",
    "        ref_dict=get_referees(lineup_soup)\n",
    "        \n",
    "        #parsing the ref_dict into the lineups dicts\n",
    "        for key in ref_dict.keys():\n",
    "            lineups[key].append(ref_dict[key])\n",
    "            \n",
    "    return lineups, lineups_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(game):\n",
    "    '''\n",
    "    input: game soup\n",
    "    ouput: date of game\n",
    "    '''\n",
    "    date=game.find('td').text\n",
    "    return date\n",
    "\n",
    "def get_team(game, team):\n",
    "    '''\n",
    "    input: game soup and team (left or right)\n",
    "    ouput: team name\n",
    "    '''\n",
    "    if team == 'left':\n",
    "        #they inverse the direction the\n",
    "        #webpages right is our left\n",
    "        class_='right-align'\n",
    "    else:\n",
    "        class_='left-align'\n",
    "        \n",
    "    team = game.find('td', class_=class_)\n",
    "    \n",
    "    return team.text\n",
    "\n",
    "def get_lineup_soup(game):\n",
    "    '''\n",
    "    input: game soup\n",
    "    output: full url of lineup webpage\n",
    "    '''\n",
    "    \n",
    "    url_end=game.find('li').find('a')['href']\n",
    "    url_start='https://www.betstudy.com'\n",
    "    full_url = url_start+url_end\n",
    "    r_lineup=requests.get(full_url)\n",
    "    lineup_soup=BeautifulSoup(r_lineup.text)\n",
    "        \n",
    "    return lineup_soup\n",
    "\n",
    "def get_lineup(lineup_soup):\n",
    "    '''\n",
    "    input: lineup page soup\n",
    "    output: two dictionaries containing lineup\n",
    "    elements for each team. First dictionary is for\n",
    "    left team, second for right.\n",
    "    '''\n",
    "    table=lineup_soup.find_all('div', class_='table-holder')[1]\n",
    "    #there are 3 table-holder div elements. 2nd is for lineups.\n",
    "    \n",
    "    #within this table there are 4 smaller tables the first\n",
    "    #and third are for the left team, the second and fourth\n",
    "    #are for the right team. The first table for each team is\n",
    "    #there starting 11, the second table for each team are their\n",
    "    #subs.\n",
    "    \n",
    "    more_tables = lineup_soup.find_all('table', 'info-table')\n",
    "    \n",
    "    left_11=more_tables[0].find_all('tr')\n",
    "    left_subs=more_tables[2].find_all('tr')\n",
    "    right_11=more_tables[1].find_all('tr')\n",
    "    right_subs=more_tables[3].find_all('tr')\n",
    "    \n",
    "    name_pos_dict={}\n",
    "    \n",
    "    for i in range(1,12): #get starting 11\n",
    "        #first element is header so skip\n",
    "        key='1starter_'+str(i)\n",
    "        left_player=left_11[i]\n",
    "        #to get players full name and position\n",
    "        #spider needs to go into their profile page.\n",
    "        name_pos_dict[key]=get_player_data(left_player)\n",
    "        \n",
    "        key='2starter_'+str(i)\n",
    "        right_player=right_11[i]\n",
    "        name_pos_dict[key]=get_player_data(right_player)\n",
    "        \n",
    "    for i in range(1, 8): #get sub 7\n",
    "        #first element is header so skip\n",
    "        key='1sub_'+str(i)\n",
    "        try:\n",
    "            left_player=left_subs[i]\n",
    "            name_pos_dict[key]=get_player_data(left_player)\n",
    "        except:\n",
    "            #not often but sometimes teams dont have 7 subs\n",
    "            #if so index will be out of range and player\n",
    "            #is np.nan and so is their position\n",
    "            name_pos_dict[key]=[np.nan, np.nan]\n",
    "        \n",
    "        \n",
    "        key='2sub_'+str(i)\n",
    "        try:\n",
    "            right_player=right_subs[i]\n",
    "            name_pos_dict[key]=get_player_data(right_player)\n",
    "        except:\n",
    "            name_pos_dict[key]=[np.nan, np.nan]\n",
    "        \n",
    "        \n",
    "    #Both dictionaries are now full, each key has \n",
    "    #a list as an element, with the first element in the\n",
    "    #list being the name of the player and the second\n",
    "    #is the position\n",
    "    \n",
    "    return name_pos_dict\n",
    "\n",
    "def get_player_soup(table_row):\n",
    "    '''\n",
    "    input: row\n",
    "    output: soup from player profile page\n",
    "    '''\n",
    "    url_end = table_row.find('a')['href']\n",
    "    url_start = 'https://www.betstudy.com'\n",
    "    full_url = url_start+url_end\n",
    "    \n",
    "    r_player = requests.get(full_url)\n",
    "    \n",
    "    player_soup = BeautifulSoup(r_player.text)\n",
    "    \n",
    "    return player_soup\n",
    "    \n",
    "def get_player_data(row):\n",
    "    '''\n",
    "    input: row\n",
    "    output: player name and position\n",
    "    '''\n",
    "    player_soup=get_player_soup(row)\n",
    "    \n",
    "    \n",
    "    player_name=player_soup.find('div', class_='compare-heading').find('h1').text\n",
    "    \n",
    "    player_position=player_soup.find('div', class_='player-bio').find_all('dd')[7].text\n",
    "    \n",
    "    return [player_name, player_position]\n",
    "\n",
    "def get_referees(lineup_soup):\n",
    "    '''\n",
    "    input: lineup soup\n",
    "    output: all 4 refs in a dict with \n",
    "    titles as keys\n",
    "    '''\n",
    "    table=lineup_soup.find_all('div', class_='table-holder')[2]\n",
    "    #there are 3 table-holder div elements. 3rd is for referees.\n",
    "    \n",
    "    #table is laid out like this\n",
    "    #                #               #\n",
    "    #   Referee      #  Assistant 1  #\n",
    "    #                #               #\n",
    "    #  Assistant 2   #    Fourth     #\n",
    "    #                #               #\n",
    "    referee_url=table.find_all('tr')[0].find_all('td')[0].find('a')['href']\n",
    "    ass1_url=table.find_all('tr')[0].find_all('td')[1].find('a')['href']\n",
    "    ass2_url=table.find_all('tr')[1].find_all('td')[0].find('a')['href']\n",
    "    fourth_url=table.find_all('tr')[1].find_all('td')[1].find('a')['href']\n",
    "    \n",
    "    ref_dict={\n",
    "        'referee':get_ref_name(referee_url),\n",
    "        'assistant_1':get_ref_name(ass1_url),\n",
    "        'assistant_2':get_ref_name(ass2_url),\n",
    "        'fourth':get_ref_name(fourth_url)\n",
    "    }\n",
    "    \n",
    "    return ref_dict\n",
    "    \n",
    "def get_ref_name(ref_url):\n",
    "    '''\n",
    "    input: ending of ref url\n",
    "    output: ref name\n",
    "    \n",
    "    gets soup then collects name\n",
    "    '''\n",
    "    url_start = 'https://www.betstudy.com'\n",
    "    full_url = url_start+ref_url\n",
    "    r_ref = requests.get(full_url)\n",
    "    ref_soup = BeautifulSoup(r_ref.text)\n",
    "    \n",
    "    ref_name=ref_soup.find('div', class_='compare-heading').find('h1').text\n",
    "    \n",
    "    return ref_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f45c9fee42a4d889dc7d7f80ff16a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=380), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "player, position = game_spider(betstudy_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_df = pd.DataFrame(player)\n",
    "position_df = pd.DataFrame(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_df.to_csv('lineup_names.csv')\n",
    "position_df.to_csv('lineup_positions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
